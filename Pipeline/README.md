# NLI BERT Fine-tuning Pipeline
## Production-Ready Docker Pipeline

Complete automated pipeline for fine-tuning BERT models on the ANLI dataset with EDA, training, and evaluation stages.

---

## ğŸ‘‹ Quick Start

### Using Docker (Recommended)

```bash
# Build the image
docker build -t anli_pipeline .

# Run complete pipeline with GPU
docker run --gpus all -v "$(pwd)/artifacts:/app/artifacts" anli_pipeline

# Run complete pipeline (CPU only)
docker run -v "$(pwd)/artifacts:/app/artifacts" anli_pipeline
```

### Pull & Run for Others

```bash
# Pull the image from Docker Hub
docker pull jayjajoo/anli_pipeline:latest

# Run with GPU and local volume mapping
docker run --gpus all -v "$(pwd)/artifacts:/app/artifacts" jayjajoo/anli_pipeline:latest

# Run CPU only
docker run -v "$(pwd)/artifacts:/app/artifacts" jayjajoo/anli_pipeline:latest
```

> For Windows PowerShell:
```powershell
docker run --gpus all -v "${PWD}/artifacts:/app/artifacts" jayjajoo/anli_pipeline:latest
```

### Using Python Directly

```bash
# Install dependencies
pip install -r requirements.txt

# Run complete pipeline
python pipeline.py

# Or run individual stages
python eda.py      # Stage 1: EDA
python train.py    # Stage 2: Training
python eval.py     # Stage 3: Evaluation
```

---

## ğŸ¯ Pipeline Architecture

### Three-Stage Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     PIPELINE.PY                             â”‚
â”‚                  (Orchestrator)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                  â”‚                  â”‚
        â–¼                  â–¼                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Stage 1    â”‚  â”‚   Stage 2    â”‚  â”‚   Stage 3    â”‚
â”‚   EDA.PY     â”‚â†’ â”‚   TRAIN.PY   â”‚â†’ â”‚   EVAL.PY    â”‚
â”‚              â”‚  â”‚              â”‚  â”‚              â”‚
â”‚ - Data stats â”‚  â”‚ - Fine-tune  â”‚  â”‚ - Test model â”‚
â”‚ - Plots      â”‚  â”‚ - Save model â”‚  â”‚ - Metrics    â”‚
â”‚ - Analysis   â”‚  â”‚ - Logging    â”‚  â”‚ - Baseline   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Stage 1: EDA (eda.py)
- Loads ANLI Round 2 dataset
- Analyzes label distribution, text lengths, word overlap
- Generates visualizations and statistics
- **Output**: `artifacts/eda/`

### Stage 2: Training (train.py)
- Loads configuration from `config.py`
- Fine-tunes BERT model on ANLI
- Implements early stopping, gradient accumulation, mixed precision
- Saves best model checkpoint
- **Output**: `artifacts/model/`, `artifacts/training/`

### Stage 3: Evaluation (eval.py)
- Loads best trained model
- Evaluates on test set
- Compares against baseline (33.7% accuracy)
- Generates confusion matrix and detailed metrics
- **Output**: `artifacts/evaluation/`

---

## ğŸ“ File Structure

```
.
â”œâ”€â”€ config.py              # Centralized configuration
â”œâ”€â”€ eda.py                 # Stage 1: Exploratory Data Analysis
â”œâ”€â”€ train.py               # Stage 2: Model training
â”œâ”€â”€ eval.py                # Stage 3: Model evaluation
â”œâ”€â”€ pipeline.py            # Main orchestrator (runs all stages)
â”œâ”€â”€ requirements.txt       # Python dependencies
â””â”€â”€ Dockerfile             # Docker container definition
```

---

## ğŸ› ï¸ Configuration

All settings are in **config.py**:

```python
# Model
MODEL_NAME = "prajjwal1/bert-tiny"  # Change to bert-base-uncased/bert-large-uncased
MAX_LENGTH = 256

# Training
BATCH_SIZE = 64
EPOCHS = 5
LEARNING_RATE = 2e-5

# Paths
ARTIFACTS_DIR = './artifacts'
MODEL_DIR = './artifacts/model'
...
```

**To change settings**: Edit `config.py` before building Docker image.

---

## ğŸ“Š Artifacts Generated

```
artifacts/
â”‚
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ pipeline_log.txt
â”‚   â””â”€â”€ pipeline_summary.json
â”‚
â”œâ”€â”€ eda/
â”‚   â”œâ”€â”€ eda_log.txt
â”‚   â”œâ”€â”€ eda_summary.json
â”‚   â”œâ”€â”€ label_distribution.json
â”‚   â”œâ”€â”€ similarity_stats_by_label.csv
â”‚   â”œâ”€â”€ premise_length_distribution.png
â”‚   â”œâ”€â”€ hypothesis_length_distribution.png
â”‚   â”œâ”€â”€ overlap_by_label.png
â”‚   â””â”€â”€ tfidf_similarity_comparison.png
â”‚
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ training_log.txt
â”‚   â”œâ”€â”€ training_history.json
â”‚   â”œâ”€â”€ training_summary.json
â”‚   â””â”€â”€ best_model.pt
â”‚
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ final_model/
â”‚   â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â”‚   â”œâ”€â”€ config.json
â”‚   â”‚   â”œâ”€â”€ tokenizer_config.json
â”‚   â”‚   â””â”€â”€ vocab.txt
â”‚   â””â”€â”€ config.json
â”‚
â””â”€â”€ evaluation/
    â”œâ”€â”€ evaluation_log.txt
    â”œâ”€â”€ evaluation_results.json
    â””â”€â”€ confusion_matrix.png
```

---

## ğŸ’£ Docker Usage

### Build Image

```bash
# Basic build
docker build -t anli_pipeline .

# Build with specific tag
docker build -t anli_pipeline:v1.0.0 .

# Build without cache
docker build --no-cache -t anli_pipeline .
```

### Run Pipeline

```bash
# Run with GPU (recommended)
docker run --gpus all -v "$(pwd)/artifacts:/app/artifacts" anli_pipeline

# Run with specific GPU
docker run --gpus '"device=0"' -v "$(pwd)/artifacts:/app/artifacts" anli_pipeline

# Run on CPU (slower)
docker run -v "$(pwd)/artifacts:/app/artifacts" anli_pipeline
```

### Run Individual Stages

```bash
# Run only EDA
docker run --rm anli_pipeline python eda.py

# Run only training
docker run --gpus all -v "$(pwd)/artifacts:/app/artifacts" anli_pipeline python train.py

# Run only evaluation
docker run --rm -v "$(pwd)/artifacts:/app/artifacts" anli_pipeline python eval.py
```

---

## ğŸ“ˆ Expected Results

### Baseline (No fine-tuning)
- Accuracy: 33.7%
- F1 Score: 0.242

### After Fine-tuning (Expected)
- Accuracy: 43-45%
- F1 Score: 0.42-0.45
- Improvement: +10-12% absolute accuracy

### Training Time (GPU)
- BERT-tiny: ~10-15 minutes
- BERT-base: ~2 hours
- BERT-large: ~3 hours

---

## ğŸ’© Troubleshooting

### CUDA Out of Memory
- Reduce `BATCH_SIZE` in `config.py`
- Increase `GRADIENT_ACCUMULATION_STEPS`

### Logs Not Showing
- Run stages individually or remove `capture_output=True`

### FileNotFoundError
- Ensure Python scripts are in the same directory

### Docker Build Fails
- Clean Docker cache: `docker system prune -a`

### Model Not Beating Baseline
- Check epochs and hyperparameters in `config.py`

---

## ğŸ—‘ï¸ Logging System

- Logs stored in `artifacts/*/` directories
- View with `cat` or `tail -f` for real-time monitoring

---

## ğŸ› ï¸ Support

- Common issues: OOM, slow training, Docker build failures, GPU detection
- Fix by adjusting config, enabling GPU, cleaning cache, or updating drivers

---

**Ready to run?**

```bash
docker run --gpus all -v "$(pwd)/artifacts:/app/artifacts" anli_pipeline
```
