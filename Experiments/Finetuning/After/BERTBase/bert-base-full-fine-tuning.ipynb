{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install -U transformers huggingface_hub","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T22:22:05.613665Z","iopub.execute_input":"2025-11-06T22:22:05.613937Z","iopub.status.idle":"2025-11-06T22:22:05.618091Z","shell.execute_reply.started":"2025-11-06T22:22:05.613915Z","shell.execute_reply":"2025-11-06T22:22:05.617315Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForSequenceClassification, \n    get_linear_schedule_with_warmup,\n    set_seed\n)\nfrom torch.optim import AdamW  # Changed: Import from torch instead of transformers\nfrom datasets import load_dataset\nfrom sklearn.metrics import accuracy_score, classification_report, f1_score, confusion_matrix\nfrom tqdm import tqdm\nimport os\nimport json\nfrom datetime import datetime","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-06T22:22:05.619247Z","iopub.execute_input":"2025-11-06T22:22:05.619981Z","iopub.status.idle":"2025-11-06T22:22:11.071568Z","shell.execute_reply.started":"2025-11-06T22:22:05.619962Z","shell.execute_reply":"2025-11-06T22:22:11.070952Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Set seeds for reproducibility\ndef set_all_seeds(seed=42):\n    set_seed(seed)\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n\nset_all_seeds(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T22:22:11.072675Z","iopub.execute_input":"2025-11-06T22:22:11.072977Z","iopub.status.idle":"2025-11-06T22:22:13.968716Z","shell.execute_reply.started":"2025-11-06T22:22:11.072961Z","shell.execute_reply":"2025-11-06T22:22:13.967929Z"}},"outputs":[{"name":"stderr","text":"2025-11-06 22:22:11.327008: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762467731.349785     335 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762467731.356735     335 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Configuration class\nclass Config:\n    # Model\n    model_name = \"bert-base-uncased\"\n    num_labels = 3\n    max_length = 256\n    \n    # Training\n    batch_size = 64\n    gradient_accumulation_steps = 2\n    epochs = 5\n    learning_rate = 2e-5\n    weight_decay = 0.01\n    warmup_ratio = 0.1\n    max_grad_norm = 1.0\n    \n    # Early stopping\n    patience = 3\n    min_delta = 0.001\n    \n    # Optimizer\n    adam_epsilon = 1e-8\n    \n    # Dropout\n    hidden_dropout_prob = 0.1\n    attention_probs_dropout_prob = 0.1\n    \n    # Paths\n    output_dir = './outputs'\n    save_dir = './fine_tuned_nli_model'\n    \n    # Device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    fp16 = torch.cuda.is_available()\n\nconfig = Config()\n\nprint(f\"Using device: {config.device}\")\nprint(f\"Mixed precision (fp16): {config.fp16}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T22:22:13.969552Z","iopub.execute_input":"2025-11-06T22:22:13.970046Z","iopub.status.idle":"2025-11-06T22:22:14.025307Z","shell.execute_reply.started":"2025-11-06T22:22:13.970026Z","shell.execute_reply":"2025-11-06T22:22:14.024144Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nMixed precision (fp16): True\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Load ANLI dataset\nprint(\"\\nLoading ANLI dataset...\")\nds = load_dataset(\"facebook/anli\")\n\n# Extract only train_r2, dev_r2, and test_r2\ntrain_data = ds['train_r2']\ndev_data = ds['dev_r2']\ntest_data = ds['test_r2']\n\nprint(f\"\\nDataset sizes:\")\nprint(f\"Train (R2): {len(train_data)}\")\nprint(f\"Dev (R2): {len(dev_data)}\")\nprint(f\"Test (R2): {len(test_data)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T22:22:14.027134Z","iopub.execute_input":"2025-11-06T22:22:14.027420Z","iopub.status.idle":"2025-11-06T22:22:15.042363Z","shell.execute_reply.started":"2025-11-06T22:22:14.027403Z","shell.execute_reply":"2025-11-06T22:22:15.041574Z"}},"outputs":[{"name":"stdout","text":"\nLoading ANLI dataset...\n\nDataset sizes:\nTrain (R2): 45460\nDev (R2): 1000\nTest (R2): 1000\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Convert to pandas DataFrames for easier handling\ndef convert_to_df(dataset):\n    return pd.DataFrame({\n        'premise': [p.lower().strip() for p in dataset['premise']],\n        'hypothesis': [h.lower().strip() for h in dataset['hypothesis']],\n        'label': dataset['label']\n    })\n    \ntrain_df = convert_to_df(train_data)\nval_df = convert_to_df(dev_data)\ntest_df = convert_to_df(test_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T22:22:15.043062Z","iopub.execute_input":"2025-11-06T22:22:15.043299Z","iopub.status.idle":"2025-11-06T22:22:17.127491Z","shell.execute_reply.started":"2025-11-06T22:22:15.043280Z","shell.execute_reply":"2025-11-06T22:22:17.126891Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Check label distribution\nprint(f\"\\nTrain label distribution:\")\nprint(train_df['label'].value_counts().sort_index())\nprint(f\"\\nVal label distribution:\")\nprint(val_df['label'].value_counts().sort_index())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T22:22:17.128059Z","iopub.execute_input":"2025-11-06T22:22:17.128274Z","iopub.status.idle":"2025-11-06T22:22:17.135786Z","shell.execute_reply.started":"2025-11-06T22:22:17.128258Z","shell.execute_reply":"2025-11-06T22:22:17.134972Z"}},"outputs":[{"name":"stdout","text":"\nTrain label distribution:\nlabel\n0    14448\n1    20959\n2    10053\nName: count, dtype: int64\n\nVal label distribution:\nlabel\n0    334\n1    333\n2    333\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Initialize tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(config.model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    config.model_name,\n    num_labels=config.num_labels,\n    hidden_dropout_prob=config.hidden_dropout_prob,\n    attention_probs_dropout_prob=config.attention_probs_dropout_prob\n)\nmodel.to(config.device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T22:22:17.136684Z","iopub.execute_input":"2025-11-06T22:22:17.136955Z","iopub.status.idle":"2025-11-06T22:22:18.533333Z","shell.execute_reply.started":"2025-11-06T22:22:17.136932Z","shell.execute_reply":"2025-11-06T22:22:18.532444Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=3, bias=True)\n)"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# Count parameters\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"\\nTotal parameters: {total_params:,}\")\nprint(f\"Trainable parameters: {trainable_params:,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T22:22:18.534104Z","iopub.execute_input":"2025-11-06T22:22:18.534334Z","iopub.status.idle":"2025-11-06T22:22:18.540400Z","shell.execute_reply.started":"2025-11-06T22:22:18.534317Z","shell.execute_reply":"2025-11-06T22:22:18.539555Z"}},"outputs":[{"name":"stdout","text":"\nTotal parameters: 109,484,547\nTrainable parameters: 109,484,547\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Custom Dataset\nclass NLIDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_length=128):\n        self.data = dataframe.reset_index(drop=True)\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        premise = str(self.data.loc[idx, 'premise']).strip()\n        hypothesis = str(self.data.loc[idx, 'hypothesis']).strip()\n        label = int(self.data.loc[idx, 'label'])\n        \n        encoding = self.tokenizer(\n            premise,\n            hypothesis,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation='longest_first',\n            return_tensors='pt'\n        )\n        \n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T22:22:18.541210Z","iopub.execute_input":"2025-11-06T22:22:18.541429Z","iopub.status.idle":"2025-11-06T22:22:18.555264Z","shell.execute_reply.started":"2025-11-06T22:22:18.541414Z","shell.execute_reply":"2025-11-06T22:22:18.554484Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Create datasets\ntrain_dataset = NLIDataset(train_df, tokenizer, config.max_length)\nval_dataset = NLIDataset(val_df, tokenizer, config.max_length)\ntest_dataset = NLIDataset(test_df, tokenizer, config.max_length)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T22:22:18.558268Z","iopub.execute_input":"2025-11-06T22:22:18.558484Z","iopub.status.idle":"2025-11-06T22:22:18.575042Z","shell.execute_reply.started":"2025-11-06T22:22:18.558469Z","shell.execute_reply":"2025-11-06T22:22:18.574302Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# DataLoaders\ntrain_loader = DataLoader(\n    train_dataset, \n    batch_size=config.batch_size, \n    shuffle=True,\n    num_workers=4,\n    pin_memory=True if config.device.type == 'cuda' else False\n)\nval_loader = DataLoader(\n    val_dataset, \n    batch_size=config.batch_size,\n    num_workers=4,\n    pin_memory=True if config.device.type == 'cuda' else False\n)\ntest_loader = DataLoader(\n    test_dataset, \n    batch_size=config.batch_size,\n    num_workers=4,\n    pin_memory=True if config.device.type == 'cuda' else False\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T22:22:18.575905Z","iopub.execute_input":"2025-11-06T22:22:18.576152Z","iopub.status.idle":"2025-11-06T22:22:18.581652Z","shell.execute_reply.started":"2025-11-06T22:22:18.576132Z","shell.execute_reply":"2025-11-06T22:22:18.580982Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Optimizer with weight decay\nno_decay = ['bias', 'LayerNorm.weight', 'LayerNorm.bias']\noptimizer_grouped_parameters = [\n    {\n        'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n        'weight_decay': config.weight_decay\n    },\n    {\n        'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n        'weight_decay': 0.0\n    }\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T22:22:18.582290Z","iopub.execute_input":"2025-11-06T22:22:18.582519Z","iopub.status.idle":"2025-11-06T22:22:18.594586Z","shell.execute_reply.started":"2025-11-06T22:22:18.582499Z","shell.execute_reply":"2025-11-06T22:22:18.594023Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"optimizer = AdamW(\n    optimizer_grouped_parameters,\n    lr=config.learning_rate,\n    eps=config.adam_epsilon\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T22:22:18.595285Z","iopub.execute_input":"2025-11-06T22:22:18.595485Z","iopub.status.idle":"2025-11-06T22:22:18.608543Z","shell.execute_reply.started":"2025-11-06T22:22:18.595466Z","shell.execute_reply":"2025-11-06T22:22:18.607891Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Scheduler\ntotal_steps = len(train_loader) * config.epochs // config.gradient_accumulation_steps\nwarmup_steps = int(total_steps * config.warmup_ratio)\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=warmup_steps,\n    num_training_steps=total_steps\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T22:22:18.609120Z","iopub.execute_input":"2025-11-06T22:22:18.609372Z","iopub.status.idle":"2025-11-06T22:22:18.619691Z","shell.execute_reply.started":"2025-11-06T22:22:18.609351Z","shell.execute_reply":"2025-11-06T22:22:18.619039Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Mixed precision scaler\nscaler = torch.cuda.amp.GradScaler() if config.fp16 else None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T22:22:18.620406Z","iopub.execute_input":"2025-11-06T22:22:18.620629Z","iopub.status.idle":"2025-11-06T22:22:18.631759Z","shell.execute_reply.started":"2025-11-06T22:22:18.620614Z","shell.execute_reply":"2025-11-06T22:22:18.631074Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_335/4018233514.py:2: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler() if config.fp16 else None\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Training function\ndef train_epoch(model, dataloader, optimizer, scheduler, device, scaler=None):\n    model.train()\n    total_loss = 0\n    predictions = []\n    true_labels = []\n    \n    optimizer.zero_grad()\n    \n    progress_bar = tqdm(dataloader, desc=\"Training\")\n    for step, batch in enumerate(progress_bar):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        \n        if scaler:\n            with torch.cuda.amp.autocast():\n                outputs = model(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    labels=labels\n                )\n                loss = outputs.loss / config.gradient_accumulation_steps\n            \n            scaler.scale(loss).backward()\n            \n            if (step + 1) % config.gradient_accumulation_steps == 0:\n                scaler.unscale_(optimizer)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n                scaler.step(optimizer)\n                scaler.update()\n                scheduler.step()\n                optimizer.zero_grad()\n        else:\n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                labels=labels\n            )\n            loss = outputs.loss / config.gradient_accumulation_steps\n            loss.backward()\n            \n            if (step + 1) % config.gradient_accumulation_steps == 0:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n                optimizer.step()\n                scheduler.step()\n                optimizer.zero_grad()\n        \n        total_loss += loss.item() * config.gradient_accumulation_steps\n        \n        logits = outputs.logits\n        preds = torch.argmax(logits, dim=1).cpu().numpy()\n        predictions.extend(preds)\n        true_labels.extend(labels.cpu().numpy())\n        \n        progress_bar.set_postfix({\n            'loss': loss.item() * config.gradient_accumulation_steps,\n            'lr': scheduler.get_last_lr()[0]\n        })\n    \n    avg_loss = total_loss / len(dataloader)\n    accuracy = accuracy_score(true_labels, predictions)\n    f1_macro = f1_score(true_labels, predictions, average='macro')\n    \n    return avg_loss, accuracy, f1_macro","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T22:22:18.632360Z","iopub.execute_input":"2025-11-06T22:22:18.632558Z","iopub.status.idle":"2025-11-06T22:22:18.644491Z","shell.execute_reply.started":"2025-11-06T22:22:18.632544Z","shell.execute_reply":"2025-11-06T22:22:18.643832Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Validation function\ndef validate(model, dataloader, device):\n    model.eval()\n    total_loss = 0\n    predictions = []\n    true_labels = []\n    \n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Validating\"):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            \n            if config.fp16:\n                with torch.cuda.amp.autocast():\n                    outputs = model(\n                        input_ids=input_ids,\n                        attention_mask=attention_mask,\n                        labels=labels\n                    )\n            else:\n                outputs = model(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    labels=labels\n                )\n            \n            loss = outputs.loss\n            logits = outputs.logits\n            \n            total_loss += loss.item()\n            \n            preds = torch.argmax(logits, dim=1).cpu().numpy()\n            predictions.extend(preds)\n            true_labels.extend(labels.cpu().numpy())\n    \n    avg_loss = total_loss / len(dataloader)\n    accuracy = accuracy_score(true_labels, predictions)\n    f1_macro = f1_score(true_labels, predictions, average='macro')\n    f1_weighted = f1_score(true_labels, predictions, average='weighted')\n    \n    return avg_loss, accuracy, f1_macro, f1_weighted, predictions, true_labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T22:22:18.645108Z","iopub.execute_input":"2025-11-06T22:22:18.645381Z","iopub.status.idle":"2025-11-06T22:22:18.658684Z","shell.execute_reply.started":"2025-11-06T22:22:18.645362Z","shell.execute_reply":"2025-11-06T22:22:18.657925Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Early stopping\nclass EarlyStopping:\n    def __init__(self, patience=3, min_delta=0.001):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        \n    def __call__(self, val_score):\n        if self.best_score is None:\n            self.best_score = val_score\n        elif val_score < self.best_score + self.min_delta:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = val_score\n            self.counter = 0\n\nearly_stopping = EarlyStopping(patience=config.patience, min_delta=config.min_delta)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T22:22:18.659417Z","iopub.execute_input":"2025-11-06T22:22:18.659627Z","iopub.status.idle":"2025-11-06T22:22:18.674856Z","shell.execute_reply.started":"2025-11-06T22:22:18.659613Z","shell.execute_reply":"2025-11-06T22:22:18.674204Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Training loop\nbest_val_f1 = 0\ntraining_history = {\n    'train_loss': [], 'train_acc': [], 'train_f1': [],\n    'val_loss': [], 'val_acc': [], 'val_f1': []\n}\n\nos.makedirs(config.output_dir, exist_ok=True)\n\nfor epoch in range(config.epochs):\n    print(f\"\\n{'='*70}\")\n    print(f\"Epoch {epoch + 1}/{config.epochs}\")\n    print(f\"{'='*70}\")\n    \n    # Train\n    train_loss, train_acc, train_f1 = train_epoch(\n        model, train_loader, optimizer, scheduler, config.device, scaler\n    )\n    print(f\"Train - Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}, F1: {train_f1:.4f}\")\n    \n    # Validate\n    val_loss, val_acc, val_f1_macro, val_f1_weighted, val_preds, val_labels = validate(\n        model, val_loader, config.device\n    )\n    print(f\"Val   - Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}\")\n    print(f\"Val F1 (Macro): {val_f1_macro:.4f}, F1 (Weighted): {val_f1_weighted:.4f}\")\n    \n    # Save metrics\n    training_history['train_loss'].append(train_loss)\n    training_history['train_acc'].append(train_acc)\n    training_history['train_f1'].append(train_f1)\n    training_history['val_loss'].append(val_loss)\n    training_history['val_acc'].append(val_acc)\n    training_history['val_f1'].append(val_f1_macro)\n    \n    # Save best model\n    if val_f1_macro > best_val_f1:\n        best_val_f1 = val_f1_macro\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict(),\n            'val_f1': val_f1_macro,\n            'val_acc': val_acc,\n        }, os.path.join(config.output_dir, 'best_model.pt'))\n        print(f\"Best model saved with F1: {val_f1_macro:.4f}\")\n    \n    # Early stopping\n    early_stopping(val_f1_macro)\n    if early_stopping.early_stop:\n        print(f\"\\nEarly stopping triggered after epoch {epoch + 1}\")\n        break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T22:22:18.675546Z","iopub.execute_input":"2025-11-06T22:22:18.675798Z","iopub.status.idle":"2025-11-06T23:02:35.848065Z","shell.execute_reply.started":"2025-11-06T22:22:18.675783Z","shell.execute_reply":"2025-11-06T23:02:35.846933Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\nEpoch 1/5\n======================================================================\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/711 [00:00<?, ?it/s]/tmp/ipykernel_335/3986490804.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\nTraining: 100%|██████████| 711/711 [07:58<00:00,  1.49it/s, loss=0.621, lr=1.78e-5]\n","output_type":"stream"},{"name":"stdout","text":"Train - Loss: 0.8723, Accuracy: 0.5836, F1: 0.5052\n","output_type":"stream"},{"name":"stderr","text":"Validating:   0%|          | 0/16 [00:00<?, ?it/s]/tmp/ipykernel_335/3010177878.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\nValidating: 100%|██████████| 16/16 [00:03<00:00,  4.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val   - Loss: 1.2974, Accuracy: 0.4130\nVal F1 (Macro): 0.3962, F1 (Weighted): 0.3963\nBest model saved with F1: 0.3962\n\n======================================================================\nEpoch 2/5\n======================================================================\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/711 [00:00<?, ?it/s]/tmp/ipykernel_335/3986490804.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\nTraining: 100%|██████████| 711/711 [07:57<00:00,  1.49it/s, loss=0.566, lr=1.33e-5]\n","output_type":"stream"},{"name":"stdout","text":"Train - Loss: 0.5531, Accuracy: 0.7758, F1: 0.7449\n","output_type":"stream"},{"name":"stderr","text":"Validating:   0%|          | 0/16 [00:00<?, ?it/s]/tmp/ipykernel_335/3010177878.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\nValidating: 100%|██████████| 16/16 [00:04<00:00,  3.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val   - Loss: 1.3550, Accuracy: 0.4360\nVal F1 (Macro): 0.4266, F1 (Weighted): 0.4266\nBest model saved with F1: 0.4266\n\n======================================================================\nEpoch 3/5\n======================================================================\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/711 [00:00<?, ?it/s]/tmp/ipykernel_335/3986490804.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\nTraining: 100%|██████████| 711/711 [07:57<00:00,  1.49it/s, loss=0.247, lr=8.9e-6] \n","output_type":"stream"},{"name":"stdout","text":"Train - Loss: 0.4110, Accuracy: 0.8397, F1: 0.8172\n","output_type":"stream"},{"name":"stderr","text":"Validating:   0%|          | 0/16 [00:00<?, ?it/s]/tmp/ipykernel_335/3010177878.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\nValidating: 100%|██████████| 16/16 [00:03<00:00,  4.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val   - Loss: 1.4944, Accuracy: 0.4410\nVal F1 (Macro): 0.4387, F1 (Weighted): 0.4387\nBest model saved with F1: 0.4387\n\n======================================================================\nEpoch 4/5\n======================================================================\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/711 [00:00<?, ?it/s]/tmp/ipykernel_335/3986490804.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\nTraining: 100%|██████████| 711/711 [07:57<00:00,  1.49it/s, loss=0.534, lr=4.46e-6]\n","output_type":"stream"},{"name":"stdout","text":"Train - Loss: 0.3231, Accuracy: 0.8797, F1: 0.8622\n","output_type":"stream"},{"name":"stderr","text":"Validating:   0%|          | 0/16 [00:00<?, ?it/s]/tmp/ipykernel_335/3010177878.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\nValidating: 100%|██████████| 16/16 [00:03<00:00,  4.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val   - Loss: 1.6463, Accuracy: 0.4330\nVal F1 (Macro): 0.4285, F1 (Weighted): 0.4285\n\n======================================================================\nEpoch 5/5\n======================================================================\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/711 [00:00<?, ?it/s]/tmp/ipykernel_335/3986490804.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\nTraining: 100%|██████████| 711/711 [07:56<00:00,  1.49it/s, loss=0.0637, lr=2.5e-8]\n","output_type":"stream"},{"name":"stdout","text":"Train - Loss: 0.2683, Accuracy: 0.9031, F1: 0.8885\n","output_type":"stream"},{"name":"stderr","text":"Validating:   0%|          | 0/16 [00:00<?, ?it/s]/tmp/ipykernel_335/3010177878.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\nValidating: 100%|██████████| 16/16 [00:04<00:00,  3.99it/s]","output_type":"stream"},{"name":"stdout","text":"Val   - Loss: 1.7620, Accuracy: 0.4370\nVal F1 (Macro): 0.4325, F1 (Weighted): 0.4325\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"checkpoint_path = os.path.join(config.output_dir, \"best_model.pt\")\n\n# Load full checkpoint (not just weights)\ncheckpoint = torch.load(checkpoint_path, weights_only=False)  # <-- allow full loading\nmodel.load_state_dict(checkpoint['model_state_dict'])\nprint(f\"\\nLoaded best model from epoch {checkpoint['epoch'] + 1}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T23:09:30.567929Z","iopub.execute_input":"2025-11-06T23:09:30.568450Z","iopub.status.idle":"2025-11-06T23:09:31.694440Z","shell.execute_reply.started":"2025-11-06T23:09:30.568423Z","shell.execute_reply":"2025-11-06T23:09:31.693607Z"}},"outputs":[{"name":"stdout","text":"\nLoaded best model from epoch 3\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# Final evaluation on test set\ntest_loss, test_acc, test_f1_macro, test_f1_weighted, test_preds, test_labels = validate(\n    model, test_loader, config.device\n)\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"FINAL TEST SET EVALUATION (R2)\")\nprint(\"=\"*70)\nprint(f\"Test Loss: {test_loss:.4f}\")\nprint(f\"Test Accuracy: {test_acc:.4f}\")\nprint(f\"Test F1 (Macro): {test_f1_macro:.4f}\")\nprint(f\"Test F1 (Weighted): {test_f1_weighted:.4f}\")\n\nprint(\"\\nClassification Report:\")\nprint(classification_report(\n    test_labels, \n    test_preds, \n    target_names=['ENTAILMENT', 'NEUTRAL', 'CONTRADICTION'],\n    digits=4\n))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T23:10:00.407969Z","iopub.execute_input":"2025-11-06T23:10:00.408275Z","iopub.status.idle":"2025-11-06T23:10:04.810136Z","shell.execute_reply.started":"2025-11-06T23:10:00.408253Z","shell.execute_reply":"2025-11-06T23:10:04.809237Z"}},"outputs":[{"name":"stderr","text":"Validating:   0%|          | 0/16 [00:00<?, ?it/s]/tmp/ipykernel_335/3010177878.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\nValidating: 100%|██████████| 16/16 [00:04<00:00,  3.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n======================================================================\nFINAL TEST SET EVALUATION (R2)\n======================================================================\nTest Loss: 1.5675\nTest Accuracy: 0.4310\nTest F1 (Macro): 0.4270\nTest F1 (Weighted): 0.4271\n\nClassification Report:\n               precision    recall  f1-score   support\n\n   ENTAILMENT     0.4108    0.5240    0.4605       334\n      NEUTRAL     0.4571    0.4474    0.4522       333\nCONTRADICTION     0.4315    0.3213    0.3683       333\n\n     accuracy                         0.4310      1000\n    macro avg     0.4331    0.4309    0.4270      1000\n weighted avg     0.4331    0.4310    0.4271      1000\n\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"print(\"\\nConfusion Matrix:\")\ncm = confusion_matrix(test_labels, test_preds)\nprint(cm)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T23:10:19.540022Z","iopub.execute_input":"2025-11-06T23:10:19.540874Z","iopub.status.idle":"2025-11-06T23:10:19.550450Z","shell.execute_reply.started":"2025-11-06T23:10:19.540833Z","shell.execute_reply":"2025-11-06T23:10:19.549604Z"}},"outputs":[{"name":"stdout","text":"\nConfusion Matrix:\n[[175  89  70]\n [113 149  71]\n [138  88 107]]\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"# Per-class accuracy\nfor i, label_name in enumerate(['ENTAILMENT', 'NEUTRAL', 'CONTRADICTION']):\n    class_acc = cm[i, i] / cm[i].sum() if cm[i].sum() > 0 else 0\n    print(f\"{label_name} Accuracy: {class_acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T23:10:21.909423Z","iopub.execute_input":"2025-11-06T23:10:21.910505Z","iopub.status.idle":"2025-11-06T23:10:21.915765Z","shell.execute_reply.started":"2025-11-06T23:10:21.910472Z","shell.execute_reply":"2025-11-06T23:10:21.914842Z"}},"outputs":[{"name":"stdout","text":"ENTAILMENT Accuracy: 0.5240\nNEUTRAL Accuracy: 0.4474\nCONTRADICTION Accuracy: 0.3213\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"# Save final model and tokenizer\nos.makedirs(config.save_dir, exist_ok=True)\nmodel.save_pretrained(config.save_dir)\ntokenizer.save_pretrained(config.save_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T23:10:25.336380Z","iopub.execute_input":"2025-11-06T23:10:25.336991Z","iopub.status.idle":"2025-11-06T23:10:26.274335Z","shell.execute_reply.started":"2025-11-06T23:10:25.336968Z","shell.execute_reply":"2025-11-06T23:10:26.273580Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"('./fine_tuned_nli_model/tokenizer_config.json',\n './fine_tuned_nli_model/special_tokens_map.json',\n './fine_tuned_nli_model/vocab.txt',\n './fine_tuned_nli_model/added_tokens.json',\n './fine_tuned_nli_model/tokenizer.json')"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"# Save training history\nwith open(os.path.join(config.save_dir, 'training_history.json'), 'w') as f:\n    json.dump(training_history, f, indent=2)\n\n# Save configuration\nwith open(os.path.join(config.save_dir, 'config.json'), 'w') as f:\n    json.dump(vars(config), f, indent=2, default=str)\n\nprint(f\"\\nModel saved to {config.save_dir}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T23:10:30.537383Z","iopub.execute_input":"2025-11-06T23:10:30.537710Z","iopub.status.idle":"2025-11-06T23:10:30.543871Z","shell.execute_reply.started":"2025-11-06T23:10:30.537689Z","shell.execute_reply":"2025-11-06T23:10:30.543194Z"}},"outputs":[{"name":"stdout","text":"\nModel saved to ./fine_tuned_nli_model\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"# Enhanced inference function\ndef predict_nli(premise, hypothesis, model, tokenizer, device, return_probs=False):\n    \"\"\"\n    Predict NLI label with optional probability scores\n    \"\"\"\n    model.eval()\n    \n    encoding = tokenizer(\n        premise,\n        hypothesis,\n        max_length=config.max_length,\n        padding='max_length',\n        truncation='longest_first',\n        return_tensors='pt'\n    )\n    \n    input_ids = encoding['input_ids'].to(device)\n    attention_mask = encoding['attention_mask'].to(device)\n    \n    with torch.no_grad():\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n        probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n        prediction = torch.argmax(logits, dim=1).item()\n    \n    labels = {0: \"ENTAILMENT\", 1: \"NEUTRAL\", 2: \"CONTRADICTION\"}\n    \n    if return_probs:\n        prob_dict = {labels[i]: float(probs[i]) for i in range(3)}\n        return labels[prediction], prob_dict\n    \n    return labels[prediction]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T23:10:37.334117Z","iopub.execute_input":"2025-11-06T23:10:37.334430Z","iopub.status.idle":"2025-11-06T23:10:37.340686Z","shell.execute_reply.started":"2025-11-06T23:10:37.334409Z","shell.execute_reply":"2025-11-06T23:10:37.339762Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"# Example predictions\nexamples = [\n    (\"A person is riding a bike.\", \"Someone is cycling.\"),\n    (\"The sky is blue.\", \"It is raining.\"),\n    (\"A dog is running in the park.\", \"An animal is outside.\")\n]\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"EXAMPLE PREDICTIONS\")\nprint(\"=\"*70)\nfor premise, hypothesis in examples:\n    prediction, probs = predict_nli(premise, hypothesis, model, tokenizer, config.device, return_probs=True)\n    print(f\"\\nPremise: {premise}\")\n    print(f\"Hypothesis: {hypothesis}\")\n    print(f\"Prediction: {prediction}\")\n    print(f\"Confidence scores: {probs}\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"TRAINING COMPLETE\")\nprint(\"=\"*70)\nprint(f\"Best Validation F1: {best_val_f1:.4f}\")\nprint(f\"Test Accuracy: {test_acc:.4f}\")\nprint(f\"Test F1 (Macro): {test_f1_macro:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T23:10:40.097305Z","iopub.execute_input":"2025-11-06T23:10:40.098099Z","iopub.status.idle":"2025-11-06T23:10:40.206523Z","shell.execute_reply.started":"2025-11-06T23:10:40.098063Z","shell.execute_reply":"2025-11-06T23:10:40.205609Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\nEXAMPLE PREDICTIONS\n======================================================================\n\nPremise: A person is riding a bike.\nHypothesis: Someone is cycling.\nPrediction: ENTAILMENT\nConfidence scores: {'ENTAILMENT': 0.8905145525932312, 'NEUTRAL': 0.045931700617074966, 'CONTRADICTION': 0.06355368345975876}\n\nPremise: The sky is blue.\nHypothesis: It is raining.\nPrediction: NEUTRAL\nConfidence scores: {'ENTAILMENT': 0.05453810095787048, 'NEUTRAL': 0.8963657021522522, 'CONTRADICTION': 0.049096208065748215}\n\nPremise: A dog is running in the park.\nHypothesis: An animal is outside.\nPrediction: ENTAILMENT\nConfidence scores: {'ENTAILMENT': 0.5328313112258911, 'NEUTRAL': 0.3241102993488312, 'CONTRADICTION': 0.14305846393108368}\n\n======================================================================\nTRAINING COMPLETE\n======================================================================\nBest Validation F1: 0.4387\nTest Accuracy: 0.4310\nTest F1 (Macro): 0.4270\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}