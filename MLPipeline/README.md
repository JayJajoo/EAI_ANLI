# NLI Model Training & Evaluation Pipeline

Complete pipeline for training and evaluating multiple Natural Language Inference (NLI) models on the ANLI dataset.

## ğŸ“ Project Structure

```
pipeline_folder/
â”œâ”€â”€ artifacts/                      # All outputs saved here
â”‚   â”œâ”€â”€ eda/                       # EDA results and plots
â”‚   â”œâ”€â”€ bert/                      # BERT model artifacts
â”‚   â”‚   â”œâ”€â”€ bert-tiny/
â”‚   â”‚   â”œâ”€â”€ bert-base-uncased/
â”‚   â”‚   â””â”€â”€ roberta-base/
â”‚   â”œâ”€â”€ logistic_regression/       # LR model artifacts
â”‚   â”‚   â”œâ”€â”€ lr-default/
â”‚   â”‚   â”œâ”€â”€ lr-l1-balanced/
â”‚   â”‚   â””â”€â”€ lr-strong-reg/
â”‚   â”œâ”€â”€ random_forest/             # RF model artifacts
â”‚   â”‚   â”œâ”€â”€ rf-default/
â”‚   â”‚   â”œâ”€â”€ rf-deep/
â”‚   â”‚   â””â”€â”€ rf-shallow/
â”‚   â”œâ”€â”€ xgboost/                   # XGBoost model artifacts
â”‚   â”‚   â”œâ”€â”€ xgb-default/
â”‚   â”‚   â”œâ”€â”€ xgb-deep/
â”‚   â”‚   â””â”€â”€ xgb-regularized/
â”‚   â””â”€â”€ pipeline/                  # Pipeline logs and reports
â”‚       â”œâ”€â”€ pipeline_log.txt
â”‚       â”œâ”€â”€ pipeline_summary.json
â”‚       â”œâ”€â”€ comparison_report.json
â”‚       â””â”€â”€ model_comparison.png
â”œâ”€â”€ models/                        # Model implementations
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ bert/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ train.py
â”‚   â”‚   â””â”€â”€ eval.py
â”‚   â”œâ”€â”€ logistic_regression/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ train.py
â”‚   â”‚   â””â”€â”€ eval.py
â”‚   â”œâ”€â”€ random_forest/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ train.py
â”‚   â”‚   â””â”€â”€ eval.py
â”‚   â””â”€â”€ xgboost/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ train.py
â”‚       â””â”€â”€ eval.py
â”œâ”€â”€ utils/                         # Shared utilities
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_loader.py            # Data loading
â”‚   â””â”€â”€ eda.py                    # EDA utilities
â”œâ”€â”€ config.py                     # All configurations
â”œâ”€â”€ pipeline.py                   # Main pipeline orchestrator
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ Dockerfile                    # Docker configuration
â””â”€â”€ README.md                     # This file
```

## ğŸš€ Quick Start

### Installation

1. **Clone/Download the project**
2. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

3. **Create empty `__init__.py` files:**
   ```bash
   touch utils/__init__.py
   touch models/__init__.py
   touch models/bert/__init__.py
   touch models/logistic_regression/__init__.py
   touch models/random_forest/__init__.py
   touch models/xgboost/__init__.py
   ```

### Running the Pipeline

**Basic usage:**
```bash
python pipeline.py
```

This will:
1. âœ… Run EDA on the ANLI dataset  
2. âœ… Train all configured models (3 BERT + 3 LR + 3 RF + 3 XGBoost = 12 models)  
3. âœ… Evaluate all models on the test set  
4. âœ… Generate comparison report and visualizations  

---

## ğŸ³ Using Docker

### Option 1: Build Locally

```bash
# Build image
docker build -t anli_pipeline .

# Run with GPU
docker run --gpus all -v "$(pwd)/artifacts:/app/artifacts" anli_pipeline

# Run without GPU (CPU only)
docker run -v "$(pwd)/artifacts:/app/artifacts" anli_pipeline
```

### Option 2: Pull from Docker Hub

You can directly pull the pre-built image instead of building it yourself:

```bash
# Pull the image
docker pull jayjajoo/anli_pipeline:latest

# Run with GPU
docker run --gpus all -v "$(pwd)/artifacts:/app/artifacts" jayjajoo/anli_pipeline:latest

# Run without GPU (CPU only)
docker run -v "$(pwd)/artifacts:/app/artifacts" jayjajoo/anli_pipeline:latest
```

All outputs (logs, metrics, visualizations, and models) will be saved to your local `artifacts/` directory.

---

## âš™ï¸ Configuration

All configurations are centralized in `config.py`:

### Adding/Modifying BERT Models

```python
# In config.py, add to BERT_CONFIGS list:
{
    'name': 'distilbert',
    'model_name': 'distilbert-base-uncased',  # Any HuggingFace model
    'max_length': 128,
    'batch_size': 32,
    'epochs': 3,
    'learning_rate': 2e-5,
}
```

### Adding/Modifying ML Models

Example for Logistic Regression:
```python
{
    'name': 'lr-custom',
    'max_iter': 1000,
    'C': 1.0,
    'solver': 'lbfgs',
    'penalty': 'l2',
    'tfidf_max_features': 10000,
    'tfidf_ngram_range': (1, 2)
}
```

---

## ğŸ“Š Output Structure

Each model configuration saves:
- `training_log.txt`  
- `evaluation_log.txt`  
- `training_results.json`  
- `evaluation_results.json`  
- `confusion_matrix.png`  
- `model.pkl` or `final_model/`

Pipeline-level outputs:
- `pipeline_log.txt`  
- `comparison_report.json`  
- `model_comparison.png`

---

## ğŸ¯ Features

- Modular model design  
- Fully configurable via `config.py`  
- Multi-model training and comparison  
- Comprehensive evaluation metrics  
- Reproducible and well-logged runs  

---

## ğŸ“ˆ Baseline Metrics

| Metric | Baseline |
|--------|-----------|
| Accuracy | 0.337 |
| F1 (Macro) | 0.242 |

---

## ğŸ› Troubleshooting

**Out of memory errors:**
- Reduce `batch_size` or `tfidf_max_features`

**Import errors:**
- Ensure `__init__.py` files exist

**Dataset issues:**
- Verify internet connection (dataset is downloaded from HuggingFace)

---

## ğŸ‘¤ Author

Jay Jajoo
